"""
Online Forecast Experiment Framework for M-Stream
åœ¨çº¿é¢„æµ‹å®éªŒæ¡†æ¶

æ ¸å¿ƒåŠŸèƒ½:
1. ç¦»çº¿é¢„è®­ç»ƒ (å¤ç”¨ Exp_Long_Term_Forecast)
2. åœ¨çº¿æµ‹è¯• (é€æ­¥æ¨ç† + åŠ¨é‡æ›´æ–°)
3. æƒŠå¥‡åº¦é—¨æ§ (è¿‡æ»¤å¼‚å¸¸æ•°æ®)
4. æ€§èƒ½ç›‘æ§å’Œå¯è§†åŒ–

Author: AI Assistant & User
Date: 2025-12-05 (Fixed v6.2)
"""

from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast
from utils.online_utils import (
    OnlineMetrics, 
    SurpriseGate,
    DelayedFeedbackBuffer,
    visualize_online_results,
    visualize_online_results_enhanced,
    visualize_memory_keys_similarity,
    print_online_summary,
    save_online_results
)
import torch
import torch.nn as nn
from torch import optim
import os
import time
import numpy as np
from tqdm import tqdm
import random
from collections import deque
from typing import Optional


class Exp_Online_Forecast(Exp_Long_Term_Forecast):
    """
    åœ¨çº¿é¢„æµ‹å®éªŒç±»
    
    ç»§æ‰¿è‡ª Exp_Long_Term_Forecastï¼Œæ·»åŠ åœ¨çº¿å­¦ä¹ åŠŸèƒ½
    """
    
    def __init__(self, args):
        super(Exp_Online_Forecast, self).__init__(args)
        
        # åœ¨çº¿å­¦ä¹ ç‰¹æœ‰å‚æ•°
        self.surprise_threshold_std = getattr(args, 'surprise_thresh', 3.0)
        self.warmup_steps = getattr(args, 'warmup_steps', 50)
        self.use_surprise_gate = getattr(args, 'use_surprise_gate', True)
        self.save_online_results = getattr(args, 'save_online_results', True)
        
        # å»¶è¿Ÿåé¦ˆå‚æ•°
        self.use_delayed_feedback = getattr(args, 'use_delayed_feedback', False)
        self.delayed_batch_size = getattr(args, 'delayed_batch_size', 8)
        self.delayed_max_wait_steps = getattr(args, 'delayed_max_wait_steps', 20)
        self.delayed_weight_decay = getattr(args, 'delayed_weight_decay', 0.05)
        self.delayed_supervised_weight = getattr(args, 'delayed_supervised_weight', 0.7)
        self.delayed_horizon = max(0, getattr(args, 'delayed_horizon', args.pred_len))
        self.online_strategy = getattr(args, 'online_strategy', 'proxy')
        self.baseline_strategies = getattr(args, 'baseline_strategies', ['static', 'proxy'])
        self.naive_ft_lr = getattr(args, 'naive_ft_lr', 1e-4)
        self.replay_buffer_size = getattr(args, 'replay_buffer_size', 256)
        self.replay_sample_size = getattr(args, 'replay_sample_size', 32)
        self.refresh_interval = getattr(args, 'refresh_interval', 200)
        self.refresh_epochs = getattr(args, 'refresh_epochs', 1)
        self.refresh_sample_limit = getattr(args, 'refresh_sample_limit', 256)
        self.checkpoint_setting = getattr(args, 'checkpoint_setting', None)
        self._supervised_optimizer = None
        self._replay_storage = deque(maxlen=self.replay_buffer_size)
        self._refresh_storage = deque(maxlen=self.refresh_sample_limit)
        self.supervised_criterion = nn.MSELoss()
    
    def _strip_timestamp(self, setting: str) -> str:
        parts = setting.split('_')
        if len(parts) >= 2 and parts[-1].isdigit() and len(parts[-1]) == 6 \
                and parts[-2].isdigit() and len(parts[-2]) == 8:
            return '_'.join(parts[:-2])
        return setting
    
    def _resolve_checkpoint_path(self, setting: str, override_setting: Optional[str] = None) -> Optional[str]:
        checkpoint_dir = self.args.checkpoints
        
        def candidate_path(name: str) -> str:
            return os.path.join(checkpoint_dir, name, 'checkpoint.pth')
        
        if override_setting:
            path = candidate_path(override_setting)
            if os.path.exists(path):
                return path
        
        direct_path = candidate_path(setting)
        if os.path.exists(direct_path):
            return direct_path
        
        base_setting = self._strip_timestamp(setting)
        if not os.path.exists(checkpoint_dir):
            return None
        
        matches = []
        for item in os.listdir(checkpoint_dir):
            item_base = self._strip_timestamp(item)
            if item_base == base_setting:
                ckpt_path = candidate_path(item)
                if os.path.exists(ckpt_path):
                    matches.append((os.path.getmtime(ckpt_path), ckpt_path))
        
        if matches:
            matches.sort(key=lambda x: x[0], reverse=True)
            return matches[0][1]
        
        return None

    def online_test(self, setting, load_checkpoint=True, checkpoint_path=None):
        """
        åœ¨çº¿æµ‹è¯•ä¸»æµç¨‹
        """
        print("\n" + "="*60)
        print("Starting Online Testing for M-Stream")
        print("="*60)
        
        # 1. åŠ è½½æ•°æ®
        test_data, test_loader = self._get_data(flag='test')
        
        # 2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        resolved_ckpt = checkpoint_path or self._resolve_checkpoint_path(setting, self.checkpoint_setting)
        if load_checkpoint:
            if resolved_ckpt and os.path.exists(resolved_ckpt):
                print(f"\nLoading pretrained model from: {resolved_ckpt}")
                self.model.load_state_dict(torch.load(resolved_ckpt))
            else:
                print(f"\nâš ï¸  Warning: Checkpoint not found for setting '{setting}'")
                print("   You may specify --checkpoint_setting <folder_name> to reuse an existing model.")
                print("   Continuing with randomly initialized weights (not recommended).")
        
        # 3. å†»ç»“ Backbone (åªæ›´æ–° Memory)
        self.model.freeze_backbone()
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self._reset_supervised_state()
        self._prepare_strategy()
        
        # 4. åˆå§‹åŒ–åœ¨çº¿å­¦ä¹ ç»„ä»¶
        metrics = OnlineMetrics()
        
        if self.use_surprise_gate:
            surprise_gate = SurpriseGate(
                threshold_std=self.surprise_threshold_std,
                warmup_steps=self.warmup_steps,
                adaptive=True,
                window_size=100
            )
            print(f"\nâœ“ Surprise Gate enabled (threshold: {self.surprise_threshold_std} std)")
        else:
            surprise_gate = None
            print("\nâœ“ Surprise Gate disabled (always update)")
        
        # åˆå§‹åŒ–å»¶è¿Ÿåé¦ˆç¼“å†²åŒº
        if self.use_delayed_feedback:
            delayed_buffer = DelayedFeedbackBuffer(
                pred_horizon=self.delayed_horizon,
                batch_size=self.delayed_batch_size,
                max_buffer_size=200,
                max_wait_steps=self.delayed_max_wait_steps,
                weight_decay=self.delayed_weight_decay,
                supervised_weight=self.delayed_supervised_weight
            )
            print(f"\nâœ“ Delayed Feedback enabled:")
            print(f"    Batch size: {self.delayed_batch_size}")
            print(f"    Max wait steps: {self.delayed_max_wait_steps}")
            print(f"    Weight decay: {self.delayed_weight_decay}")
            print(f"    Supervised weight: {self.delayed_supervised_weight}")
        else:
            delayed_buffer = None
            print("\nâœ“ Delayed Feedback disabled (Proxy Loss only)")
        
        print(f"âœ“ Test samples: {len(test_data)}")
        print(f"âœ“ Batch size: {self.args.batch_size}")
        print(f"âœ“ Online Strategy: {self.online_strategy}")
        print("\nStarting online inference...\n")
        
        # ä¿å­˜é¢„æµ‹å€¼å’ŒçœŸå®å€¼ç”¨äº CSV å¯¼å‡º
        all_predictions = []
        all_targets = []
        
        # 5. é€æ­¥åœ¨çº¿æ¨ç†
        enable_proxy_updates = self.online_strategy in ['proxy', 'proxy_delayed', 'proxy_supervised']
        
        with tqdm(total=len(test_loader), desc="Online Testing") as pbar:
            for step, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):
                # ç§»åŠ¨åˆ°è®¾å¤‡
                batch_x = batch_x.float().to(self.device)
                batch_y = batch_y.float().to(self.device)
                
                # [Fix 1] åˆå§‹åŒ–æ›´æ–°è®¡æ—¶å™¨ (é˜²æ­¢ NameError)
                t_update_start = time.time()
                
                # ========== Step 1: Pre-Hoc TTT (å…ˆè‡ªç›‘ç£æ›´æ–°) ==========
                if enable_proxy_updates:
                    # 1. ä¸´æ—¶å‰å‘ä¼ æ’­è·å–ç‰¹å¾ (å¼€å¯æ¢¯åº¦)
                    with torch.enable_grad():
                        # è°ƒç”¨ model è·å– enc_outï¼Œæ­¤æ—¶ä¸è®¡ç®—æœ€ç»ˆé¢„æµ‹
                        # æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„ enc_out æ˜¯å¸¦æœ‰æ¢¯åº¦çš„
                        _, enc_out_tmp, _ = self.model(batch_x, mode='online')
                        
                        # 2. è®¡ç®— Proxy Loss
                        proxy_loss = self.model.memory.compute_proxy_loss(enc_out_tmp)
                        proxy_loss_value = proxy_loss.item()
                        
                        # 3. Surprise Gate åˆ¤æ–­
                        if self.use_surprise_gate:
                            should_update, gate_info = surprise_gate.should_update(proxy_loss_value)
                            is_anomaly = gate_info.get('is_anomaly', False)
                        else:
                            should_update = True
                            is_anomaly = False
                        
                        # 4. åŠ¨é‡æ›´æ–°
                        if should_update:
                            self.model.memory.update_with_momentum(proxy_loss)
                else:
                    proxy_loss_value = 0.0
                    should_update = False
                    is_anomaly = False

                # [Fix 2] è®°å½• Proxy æ›´æ–°è€—æ—¶
                t_update = time.time() - t_update_start

                # ========== Step 2: æ­£å¼å‰å‘é¢„æµ‹ (ä½¿ç”¨æ›´æ–°åçš„å‚æ•°) ==========
                # æ³¨æ„ï¼šå¦‚æœæ˜¯ Proxy æ¨¡å¼ï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯åˆšåˆšæ›´æ–°è¿‡çš„ Memory
                t_start = time.time()
                with torch.no_grad():
                    pred, enc_out, debug_info = self.model(batch_x, mode='online')
                t_inference = time.time() - t_start
                
                # è®°å½• Gate Value
                if 'gate_value' in debug_info:
                    metrics.record_gate(debug_info['gate_value'])
                
                # æå–ç›®æ ‡
                f_dim = -1 if self.args.features == 'MS' else 0
                target = batch_y[:, -self.args.pred_len:, f_dim:]
                pred = pred[:, -self.args.pred_len:, f_dim:]
                
                # å¤„ç†ç›‘ç£æ›´æ–°çŠ¶æ€ (ä»…ç”¨äºç»Ÿè®¡)
                supervised_updated = False
                supervised_loss_value = 0.0
                
                # ========== Step 3: å»¶è¿Ÿåé¦ˆæˆ–å…¶ä»–ç­–ç•¥ ==========
                t_supervised_start = time.time()
                
                if self.use_delayed_feedback and delayed_buffer is not None:
                    # æ·»åŠ æ ·æœ¬å’Œæ ‡ç­¾åˆ°ç¼“å†²åŒº
                    delayed_buffer.add_sample(step, batch_x, enc_out)
                    delayed_buffer.add_label(step, target)
                    delayed_buffer.advance_time(step)
                    
                    if delayed_buffer.should_update(step, is_anomaly):
                        batch_data, weights = delayed_buffer.get_batch()
                        if batch_data is not None:
                            delayed_loss = self._supervised_update(
                                batch_data, 
                                weights,
                                self.delayed_supervised_weight
                            )
                            supervised_loss_value = delayed_loss
                            supervised_updated = True
                
                # [Fix 3] ä¿®å¤å³æ—¶ç›‘ç£æ¨¡å¼ (proxy_supervised)
                elif self.online_strategy == 'proxy_supervised':
                    # å¿…é¡»é‡æ–°è®¡ç®—å¸¦æ¢¯åº¦çš„é¢„æµ‹ï¼Œå› ä¸º Step 2 æ˜¯ no_grad çš„
                    with torch.enable_grad():
                        pred_grad, _, _ = self.model(batch_x, mode='online')
                        pred_grad = pred_grad[:, -self.args.pred_len:, f_dim:]
                        
                        # è®¡ç®—ç›‘ç£ Loss å¹¶æ›´æ–° Memory
                        supervised_loss = self.supervised_criterion(pred_grad, target)
                        self.model.memory.update_with_momentum(supervised_loss)
                        
                    supervised_loss_value = supervised_loss.item()
                    supervised_updated = True

                elif self.online_strategy == 'naive_ft':
                    supervised_loss_value = self._naive_finetune_step(pred, target)
                    supervised_updated = supervised_loss_value > 0
                
                elif self.online_strategy == 'replay':
                     self._store_replay_sample(batch_x, target)
                     replay_loss = self._replay_update()
                     if replay_loss > 0:
                        supervised_loss_value = replay_loss
                        supervised_updated = True

                elif self.online_strategy == 'refresh':
                    self._store_refresh_sample(batch_x, target)
                    # Refresh é€»è¾‘é€šå¸¸æ˜¯æ¯ N æ­¥æ‰§è¡Œä¸€æ¬¡ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
                    if (step + 1) % max(1, self.refresh_interval) == 0:
                        refresh_loss = self._offline_refresh_update()
                        if refresh_loss > 0:
                            supervised_loss_value = refresh_loss
                            supervised_updated = True

                t_supervised = time.time() - t_supervised_start

                # ä¿å­˜é¢„æµ‹ç»“æœ
                all_predictions.append(pred.detach().cpu().numpy())
                all_targets.append(target.detach().cpu().numpy())
                
                # è®°å½•æ‰€æœ‰æŒ‡æ ‡
                metrics.record_prediction(pred, target)
                metrics.record_update(should_update, proxy_loss_value, supervised_updated, supervised_loss_value)
                metrics.record_time(t_inference, t_update, t_supervised)
                
                pbar.update(1)
                if (step + 1) % 100 == 0:
                    current_metrics = metrics.compute()
                    postfix_dict = {
                        'MSE': f"{current_metrics['mse']:.4f}",
                        'Proxy%': f"{current_metrics['update_rate']*100:.1f}",
                        'ProxyL': f"{proxy_loss_value:.4f}"
                    }
                    pbar.set_postfix(postfix_dict)
        
        # 6. è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        print("\n" + "="*60)
        print("Online Testing Completed!")
        print("="*60)
        
        final_metrics = metrics.compute()
        print_online_summary(final_metrics)
        
        # 7. è·å–è®°å¿†æ¨¡å—ç»Ÿè®¡
        memory_stats = self.model.get_memory_statistics()
        print("\nğŸ“Š Memory Module Statistics:")
        for key, value in memory_stats.items():
            print(f"  {key}: {value:.6f}")
        
        # 8. ä¿å­˜ç»“æœ
        if self.save_online_results:
            save_suffix = []
            if self.args.mode == 'train_and_test':
                save_suffix.append('train')
            elif self.args.mode in ['test_only', 'baseline']:
                save_suffix.append(self.online_strategy)
            elif self.args.mode in ['compare', 'ablation']:
                save_suffix.append(self.online_strategy)
            save_tag = f"{setting}_{'_'.join(save_suffix)}" if save_suffix else setting
            save_dir = os.path.join('./results', save_tag)
            os.makedirs(save_dir, exist_ok=True)
            
            trajectory = metrics.get_trajectory()
            
            # ä¿å­˜æŒ‡æ ‡å’Œè½¨è¿¹ï¼ˆåŒ…å« CSVï¼‰
            channel_names = None
            if self.args.features == 'MS':
                channel_names = [self.args.target]
            save_online_results(
                final_metrics, 
                trajectory, 
                save_dir, 
                'online_test',
                predictions=all_predictions,
                targets=all_targets,
                channel_names=channel_names
            )
            
            # ä½¿ç”¨å¢å¼ºç‰ˆå¯è§†åŒ–ï¼ˆç”Ÿæˆå¤šä¸ªå›¾è¡¨ï¼‰
            visualize_online_results_enhanced(
                metrics, 
                save_dir=save_dir, 
                setting='online_test',
                show=False
            )
            
            # åŒæ—¶ä¿ç•™åŸæœ‰çš„å•ä¸€å›¾è¡¨ï¼ˆå‘åå…¼å®¹ï¼‰
            fig_path = os.path.join(save_dir, 'online_test_visualization.png')
            visualize_online_results(metrics, save_path=fig_path, show=False)
            
            # v6.0: å¯è§†åŒ– Memory Keys ç›¸ä¼¼åº¦çŸ©é˜µ
            if hasattr(self.model, 'memory'):
                memory_vis_path = os.path.join(save_dir, 'memory_keys_similarity.png')
                sim_stats = visualize_memory_keys_similarity(
                    self.model, 
                    save_path=memory_vis_path, 
                    show=False
                )
                if sim_stats:
                    print(f"\nğŸ” Memory Similarity Analysis:")
                    print(f"  Orthogonality Score: {sim_stats['orthogonality_score']:.4f}")
                    print(f"  Off-Diagonal Mean: {sim_stats['off_diagonal_mean']:.4f}")
            
            print(f"\nâœ“ Results saved to {save_dir}")
        
        # 9. v6.0: æ‰“å° Memory ç»Ÿè®¡ä¿¡æ¯
        if hasattr(self.model, 'memory'):
            mem_stats = self.model.get_memory_statistics()
            print("\nğŸ§  Memory Statistics (v6.0):")
            print(f"  Total Updates: {mem_stats.get('update_count', 0)}")
            print(f"  Avg Proxy Loss: {mem_stats.get('avg_proxy_loss', 0.0):.6f}")
            print(f"  Alpha (Gate): {mem_stats.get('alpha_clamped', 0.0):.4f}")
            print(f"  Keys Norm: {mem_stats.get('keys_norm', 0.0):.4f}")
            print(f"  Values Norm: {mem_stats.get('values_norm', 0.0):.4f}")
            
            # v6.0: æ­£äº¤æ€§ç»Ÿè®¡
            if 'keys_orthogonality' in mem_stats:
                orth_val = mem_stats['keys_orthogonality']
                print(f"  Keys Orthogonality: {orth_val:.6f} (closer to 0 is better)")
                # è¯„ä¼°æ­£äº¤æ€§è´¨é‡
                if orth_val < 0.1:
                    quality = "âœ“ Excellent"
                elif orth_val < 0.3:
                    quality = "â—‹ Good"
                elif orth_val < 0.5:
                    quality = "â–³ Fair"
                else:
                    quality = "âœ— Poor (Mode Collapse Risk)"
                print(f"  Orthogonality Quality: {quality}")
        
        # 10. æ‰“å°å»¶è¿Ÿåé¦ˆç»Ÿè®¡ (å¦‚æœå¯ç”¨)
        if self.use_delayed_feedback and delayed_buffer is not None:
            buffer_stats = delayed_buffer.get_statistics()
            print("\nğŸ“¦ Delayed Feedback Buffer Statistics:")
            for key, value in buffer_stats.items():
                if isinstance(value, float):
                    print(f"  {key}: {value:.2f}")
                else:
                    print(f"  {key}: {value}")
        
        return final_metrics
    
    def _supervised_update(
        self, 
        batch_data: list, 
        weights: np.ndarray,
        supervised_weight: float
    ) -> float:
        """
        ä½¿ç”¨å»¶è¿Ÿåˆ°è¾¾çš„çœŸå®æ ‡ç­¾è¿›è¡Œç›‘ç£æ›´æ–°
        """
        if not batch_data:
            return 0.0
        
        total_loss = 0.0
        criterion = nn.MSELoss(reduction='none')
        
        for idx, (sample_step, batch_x, batch_y, enc_out) in enumerate(batch_data):
            # 1. å‰å‘ä¼ æ’­ (é‡æ–°è®¡ç®—é¢„æµ‹, å¼€å¯æ¢¯åº¦)
            with torch.enable_grad():
                pred, _, _ = self.model(batch_x, mode='online')
                
                # æå–ç›®æ ‡
                f_dim = -1 if self.args.features == 'MS' else 0
                target = batch_y[:, -self.args.pred_len:, f_dim:]
                pred = pred[:, -self.args.pred_len:, f_dim:]
                
                # 2. è®¡ç®—ç›‘ç£æŸå¤± (åŠ æƒ)
                sample_loss = criterion(pred, target).mean()
                weighted_loss = sample_loss * weights[idx]
                
                # 3. è®¡ç®— Proxy Loss (ç”¨äºæ··åˆæ¢¯åº¦)
                # æ³¨æ„ï¼šenc_out ä¹Ÿæ˜¯ä»å¸¦æ¢¯åº¦çš„ forward æ¥çš„
                # ä½†ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬è¿™é‡Œä¸»è¦å…³æ³¨ supervised
                # å¦‚æœæƒ³æ··åˆ proxyï¼Œéœ€è¦ä¿è¯ enc_out ä¹Ÿæœ‰æ¢¯åº¦
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šåªç”¨ supervised
                
                # 4. åŠ¨é‡æ›´æ–°
                self.model.memory.update_with_momentum(weighted_loss)
            
            total_loss += sample_loss.item()
        
        avg_supervised_loss = total_loss / len(batch_data)
        return avg_supervised_loss
    
    def _reset_supervised_state(self):
        self._supervised_optimizer = None
        self._replay_storage.clear()
        self._refresh_storage.clear()
    
    def _prepare_strategy(self):
        strategy = getattr(self, 'online_strategy', 'proxy')
        
        if strategy == 'proxy_delayed':
            self.use_delayed_feedback = True
        elif strategy in ['naive_ft', 'replay', 'refresh', 'static']:
            self.use_delayed_feedback = False
        elif strategy == 'proxy_supervised':  # <--- æ–°å¢
            self.use_delayed_feedback = False
            
        if strategy in ['naive_ft', 'replay', 'refresh']:
            self.use_surprise_gate = False
            for param in self.model.memory.parameters():
                param.requires_grad = False
            for param in self.model.head_memory.parameters():
                param.requires_grad = True
        elif strategy == 'static':
            self.use_surprise_gate = False
            for param in self.model.head_memory.parameters():
                param.requires_grad = False
        else: # proxy, proxy_delayed, proxy_supervised
            for param in self.model.memory.parameters():
                param.requires_grad = True
            for param in self.model.head_memory.parameters():
                param.requires_grad = True
    
    def _get_supervised_optimizer(self):
        if self._supervised_optimizer is None:
            params = [p for p in self.model.head_memory.parameters() if p.requires_grad]
            if not params:
                return None
            self._supervised_optimizer = optim.Adam(params, lr=self.naive_ft_lr)
        return self._supervised_optimizer
    
    def _naive_finetune_step(self, pred, target):
        optimizer = self._get_supervised_optimizer()
        if optimizer is None:
            return 0.0
        loss = self.supervised_criterion(pred, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        return loss.item()
    
    def _store_replay_sample(self, batch_x, target):
        self._replay_storage.append((
            batch_x.detach().cpu(),
            target.detach().cpu()
        ))
    
    def _replay_update(self):
        if len(self._replay_storage) < max(2, self.replay_sample_size):
            return 0.0
        sample_size = min(self.replay_sample_size, len(self._replay_storage))
        indices = np.random.choice(len(self._replay_storage), sample_size, replace=False)
        batch_x = torch.cat([self._replay_storage[i][0] for i in indices], dim=0).to(self.device)
        batch_y = torch.cat([self._replay_storage[i][1] for i in indices], dim=0).to(self.device)
        pred, _, _ = self.model(batch_x, mode='online')
        f_dim = -1 if self.args.features == 'MS' else 0
        target = batch_y[:, -self.args.pred_len:, f_dim:]
        pred = pred[:, -self.args.pred_len:, f_dim:]
        optimizer = self._get_supervised_optimizer()
        if optimizer is None:
            return 0.0
        loss = self.supervised_criterion(pred, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        return loss.item()
    
    def _store_refresh_sample(self, batch_x, target):
        self._refresh_storage.append((
            batch_x.detach().cpu(),
            target.detach().cpu()
        ))
    
    def _offline_refresh_update(self):
        if not self._refresh_storage:
            return 0.0
        optimizer = self._get_supervised_optimizer()
        if optimizer is None:
            return 0.0
        samples = list(self._refresh_storage)
        losses = []
        for _ in range(max(1, self.refresh_epochs)):
            random.shuffle(samples)
            for batch_x_cpu, batch_y_cpu in samples:
                batch_x = batch_x_cpu.to(self.device)
                batch_y = batch_y_cpu.to(self.device)
                pred, _, _ = self.model(batch_x, mode='online')
                f_dim = -1 if self.args.features == 'MS' else 0
                target = batch_y[:, -self.args.pred_len:, f_dim:]
                pred = pred[:, -self.args.pred_len:, f_dim:]
                loss = self.supervised_criterion(pred, target)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                losses.append(loss.item())
        return float(np.mean(losses)) if losses else 0.0
    
    def compare_static_vs_online(self, setting):
        """
        å¯¹æ¯”é™æ€æ¨¡å‹ vs åœ¨çº¿å­¦ä¹ æ¨¡å‹
        """
        print("\n" + "="*60)
        print("Comparing Static Model vs Online Learning Model")
        print("="*60)
        
        checkpoint_path = self._resolve_checkpoint_path(setting, self.checkpoint_setting)
        if not checkpoint_path:
            print(f"\nâŒ Error: checkpoint not found for setting '{setting}'.")
            print("   Please pass --checkpoint_setting <existing_folder> or rerun training.")
            return {}
        
        # 1. æµ‹è¯•é™æ€æ¨¡å‹ (ä¸æ›´æ–°)
        print("\n[1/2] Testing Static Model (No Updates)...")
        static_setting = f"{setting}_static"
        self.online_strategy = 'static'
        self.args.online_strategy = 'static'
        static_metrics = self.online_test(static_setting, load_checkpoint=True, checkpoint_path=checkpoint_path)
        
        # 2. æµ‹è¯•åœ¨çº¿å­¦ä¹ æ¨¡å‹
        print("\n[2/2] Testing Online Learning Model (With Updates)...")
        online_setting = f"{setting}_proxy"
        self.online_strategy = 'proxy'
        self.args.online_strategy = 'proxy'
        online_metrics = self.online_test(online_setting, load_checkpoint=True, checkpoint_path=checkpoint_path)
        
        # 3. å¯¹æ¯”ç»“æœ
        print("\n" + "="*60)
        print("Comparison Results")
        print("="*60)
        
        comparison = {
            'static': static_metrics,
            'online': online_metrics,
            'improvement': {}
        }
        
        # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
        for key in ['mse', 'mae', 'rmse']:
            if key in static_metrics and key in online_metrics:
                static_val = static_metrics[key]
                online_val = online_metrics[key]
                improvement = (static_val - online_val) / static_val * 100
                comparison['improvement'][key] = improvement
        
        print("\nğŸ“Š Performance Comparison:")
        print(f"{'Metric':<10} {'Static':<12} {'Online':<12} {'Improvement':<12}")
        print("-" * 50)
        for key in ['mse', 'mae', 'rmse']:
            static_val = static_metrics[key]
            online_val = online_metrics[key]
            improvement = comparison['improvement'][key]
            print(f"{key.upper():<10} {static_val:<12.6f} {online_val:<12.6f} {improvement:>+10.2f}%")
        
        print("\n" + "="*60)
        
        return comparison
    
    def ablation_study(self, setting):
        """
        æ¶ˆèå®éªŒ: æµ‹è¯•ä¸åŒé…ç½®çš„å½±å“
        """
        print("\n" + "="*60)
        print("Ablation Study for M-Stream")
        print("="*60)
        
        results = {}
        
        # ä¿å­˜åŸå§‹é…ç½®
        original_beta = self.model.memory.beta
        original_use_gate = self.use_surprise_gate
        
        checkpoint_path = self._resolve_checkpoint_path(setting, self.checkpoint_setting)
        if not checkpoint_path:
            print(f"\nâŒ Error: No checkpoint available for setting '{setting}'.")
            print("   Please provide --checkpoint_setting <folder> or run training first.")
            return results
        
        # æµ‹è¯•ä¸åŒçš„åŠ¨é‡å› å­
        beta_values = [0.0, 0.5, 0.9, 0.95]
        
        for beta in beta_values:
            print(f"\n[Testing] Momentum Beta = {beta}")
            
            try:
                # é‡æ–°åŠ è½½æ¨¡å‹å¹¶é‡ç½®çŠ¶æ€
                checkpoint = torch.load(checkpoint_path, map_location=self.device)
                self.model.load_state_dict(checkpoint)
                self.model.to(self.device)
                self.model.eval()
                
                # é‡ç½® momentum buffer
                self.model.memory.reset_momentum()
                
                # è®¾ç½®åŠ¨é‡å› å­
                self.model.memory.beta = float(beta)
                
                # è¿è¡Œæµ‹è¯•
                metrics = self.online_test(setting, load_checkpoint=False)
                results[f'beta_{beta}'] = metrics
                
            except Exception as e:
                print(f"\nâŒ Error during testing with beta={beta}: {str(e)}")
                results[f'beta_{beta}'] = {'mse': float('inf')}
        
        # æµ‹è¯• Surprise Gate
        try:
            print(f"\n[Testing] Without Surprise Gate")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.model.load_state_dict(checkpoint)
            self.model.to(self.device)
            self.model.eval()
            self.model.memory.reset_momentum()
            self.model.memory.beta = original_beta
            self.use_surprise_gate = False
            metrics_no_gate = self.online_test(setting, load_checkpoint=False)
            results['no_surprise_gate'] = metrics_no_gate
        except Exception as e:
            print(f"\nâŒ Error during testing without surprise gate: {str(e)}")
            results['no_surprise_gate'] = {'mse': float('inf')}
        
        try:
            print(f"\n[Testing] With Surprise Gate")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.model.load_state_dict(checkpoint)
            self.model.to(self.device)
            self.model.eval()
            self.model.memory.reset_momentum()
            self.use_surprise_gate = True
            metrics_with_gate = self.online_test(setting, load_checkpoint=False)
            results['with_surprise_gate'] = metrics_with_gate
        except Exception as e:
            print(f"\nâŒ Error during testing with surprise gate: {str(e)}")
            results['with_surprise_gate'] = {'mse': float('inf')}
        
        # æ¢å¤åŸå§‹é…ç½®
        self.model.memory.beta = original_beta
        self.use_surprise_gate = original_use_gate
        
        print("\n" + "="*60)
        return results
    
    def run_baselines(self, setting):
        """
        æŒ‰ç…§ baseline ç­–ç•¥åˆ—è¡¨ä¾æ¬¡è¿è¡Œåœ¨çº¿æµ‹è¯•
        """
        checkpoint_path = self._resolve_checkpoint_path(setting, self.checkpoint_setting)
        if not checkpoint_path:
            print(f"\nâŒ Error: checkpoint not found for setting '{setting}'.")
            return {}
        
        results = {}
        for strategy in self.baseline_strategies:
            tag = strategy.strip()
            if not tag:
                continue
            print("\n" + "="*40)
            print(f"Running baseline strategy: {tag}")
            print("="*40)
            self.online_strategy = tag
            self.args.online_strategy = tag
            strategy_setting = f"{setting}_{tag}"
            metrics = self.online_test(strategy_setting, load_checkpoint=True, checkpoint_path=checkpoint_path)
            results[tag] = metrics
        return results
