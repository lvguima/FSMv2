"""
Online Learning Utilities for M-Stream
åœ¨çº¿å­¦ä¹ å·¥å…·å‡½æ•°

åŒ…å«:
1. OnlineMetrics - åœ¨çº¿è¯„ä¼°æŒ‡æ ‡æ”¶é›†
2. SurpriseGate - æƒŠå¥‡åº¦é—¨æ§
3. å¯è§†åŒ–å·¥å…·
4. å…¶ä»–è¾…åŠ©å‡½æ•°

Author: AI Assistant & User
Date: 2025-11-19
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional
import time
import os


class OnlineMetrics:
    """
    åœ¨çº¿è¯„ä¼°æŒ‡æ ‡æ”¶é›†å™¨
    
    è·Ÿè¸ª:
    - é¢„æµ‹æ€§èƒ½ (MSE, MAE, RMSE)
    - æ›´æ–°ç»Ÿè®¡ (æ›´æ–°ç‡, å¼‚å¸¸ç‡)
    - æ—¶é—´æ€§èƒ½ (æ¨ç†æ—¶é—´, æ›´æ–°æ—¶é—´)
    """
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """é‡ç½®æ‰€æœ‰ç»Ÿè®¡"""
        # é¢„æµ‹è¯¯å·®
        self.predictions = []
        self.targets = []
        self.mse_list = []
        self.mae_list = []
        
        # æ›´æ–°ç»Ÿè®¡
        self.update_flags = []  # True: æ›´æ–°, False: è·³è¿‡
        self.proxy_losses = []
        self.anomaly_indices = []
        
        # ç›‘ç£æ›´æ–°ç»Ÿè®¡ (Delayed Feedback)
        self.supervised_update_flags = []  # True: ç›‘ç£æ›´æ–°, False: ä»… Proxy æ›´æ–°
        self.supervised_losses = []  # ç›‘ç£æŸå¤±å€¼
        
        # æ—¶é—´ç»Ÿè®¡
        self.inference_times = []
        self.update_times = []
        self.supervised_update_times = []
        self.total_times = []
        
        # é—¨æ§ç»Ÿè®¡
        self.gate_values = []
        
        # æ­¥æ•°
        self.total_steps = 0
    
    def record_prediction(self, pred: torch.Tensor, target: torch.Tensor):
        """
        è®°å½•é¢„æµ‹ç»“æœ
        
        Args:
            pred: [B, H, C]
            target: [B, H, C]
        """
        pred_np = pred.detach().cpu().numpy()
        target_np = target.detach().cpu().numpy()
        
        self.predictions.append(pred_np)
        self.targets.append(target_np)
        
        # è®¡ç®—å½“å‰æ­¥çš„è¯¯å·®
        mse = np.mean((pred_np - target_np) ** 2)
        mae = np.mean(np.abs(pred_np - target_np))
        
        self.mse_list.append(mse)
        self.mae_list.append(mae)
    
    def record_update(self, updated: bool, proxy_loss: float, supervised: bool = False, supervised_loss: float = 0.0):
        """
        è®°å½•æ›´æ–°çŠ¶æ€
        
        Args:
            updated: æ˜¯å¦æ‰§è¡Œäº† Proxy æ›´æ–°
            proxy_loss: ä»£ç†æŸå¤±å€¼
            supervised: æ˜¯å¦æ‰§è¡Œäº†ç›‘ç£æ›´æ–°
            supervised_loss: ç›‘ç£æŸå¤±å€¼
        """
        self.update_flags.append(updated)
        self.proxy_losses.append(proxy_loss)
        self.supervised_update_flags.append(supervised)
        self.supervised_losses.append(supervised_loss)
        
        if not updated:
            self.anomaly_indices.append(self.total_steps)
        
        self.total_steps += 1
    
    def record_time(self, inference_time: float, update_time: float = 0.0, supervised_update_time: float = 0.0):
        """
        è®°å½•æ—¶é—´æ¶ˆè€—
        
        Args:
            inference_time: æ¨ç†æ—¶é—´ (ç§’)
            update_time: Proxy æ›´æ–°æ—¶é—´ (ç§’)
            supervised_update_time: ç›‘ç£æ›´æ–°æ—¶é—´ (ç§’)
        """
        self.inference_times.append(inference_time)
        self.update_times.append(update_time)
        self.supervised_update_times.append(supervised_update_time)
        self.total_times.append(inference_time + update_time + supervised_update_time)
        
    def record_gate(self, gate_value: float):
        """
        è®°å½•é—¨æ§å€¼
        
        Args:
            gate_value: é—¨æ§å€¼ (0~1)
        """
        if hasattr(gate_value, "detach"):
            try:
                gate_value = gate_value.detach().cpu().item()
            except Exception:
                gate_value = float(gate_value.detach().cpu().numpy())
        self.gate_values.append(float(gate_value))
    
    def compute(self) -> Dict[str, float]:
        """
        è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        
        Returns:
            metrics: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
        """
        # é¢„æµ‹æ€§èƒ½
        mse = np.mean(self.mse_list)
        mae = np.mean(self.mae_list)
        rmse = np.sqrt(mse)

        # é¢å¤–è¯„ä¼°æŒ‡æ ‡ï¼ˆåŸºäºå…¨é‡é¢„æµ‹ï¼‰
        rse = 0.0
        r2 = 0.0
        mape = 0.0
        if len(self.predictions) > 0 and len(self.targets) > 0:
            preds_all = np.concatenate(self.predictions, axis=0)
            targets_all = np.concatenate(self.targets, axis=0)

            residual = preds_all - targets_all
            sse = np.sum(residual ** 2)
            sst = np.sum((targets_all - np.mean(targets_all)) ** 2) + 1e-8  # é¿å…é™¤é›¶

            rse = float(np.sqrt(sse) / np.sqrt(sst)) if sst > 0 else 0.0
            r2 = float(1.0 - sse / sst) if sst > 0 else 0.0

            # ä¸ utils/metrics.py ä¿æŒä¸€è‡´ï¼šMAPE = mean(|true - pred| / true)
            denom = targets_all + 1e-6  # é¿å…é™¤é›¶ï¼Œä¿ç•™ç¬¦å·ä¸€è‡´æ€§
            mape = float(np.mean(np.abs(residual / denom)))
        
        # æ›´æ–°ç»Ÿè®¡
        update_rate = np.mean(self.update_flags) if self.update_flags else 0.0
        anomaly_rate = 1.0 - update_rate
        avg_proxy_loss = np.mean(self.proxy_losses) if self.proxy_losses else 0.0
        
        # ç›‘ç£æ›´æ–°ç»Ÿè®¡
        supervised_update_rate = np.mean(self.supervised_update_flags) if self.supervised_update_flags else 0.0
        supervised_losses_filtered = [l for l in self.supervised_losses if l > 0]
        avg_supervised_loss = np.mean(supervised_losses_filtered) if supervised_losses_filtered else 0.0
        
        # æ—¶é—´ç»Ÿè®¡
        avg_inference_time = np.mean(self.inference_times) if self.inference_times else 0.0
        avg_update_time = np.mean(self.update_times) if self.update_times else 0.0
        avg_supervised_update_time = np.mean([t for t in self.supervised_update_times if t > 0]) if any(t > 0 for t in self.supervised_update_times) else 0.0
        total_time = avg_inference_time + avg_update_time + avg_supervised_update_time
        latency_p95 = np.percentile(self.total_times, 95) if self.total_times else 0.0
        throughput = 1.0 / np.mean(self.total_times) if self.total_times and np.mean(self.total_times) > 0 else 0.0
        
        # ç¨³å®šæ€§æŒ‡æ ‡
        mse_volatility = float(np.std(self.mse_list)) if len(self.mse_list) > 1 else 0.0
        drift_window = max(5, len(self.mse_list) // 10)
        if drift_window > 0 and len(self.mse_list) >= 2 * drift_window:
            start_mean = np.mean(self.mse_list[:drift_window])
            end_mean = np.mean(self.mse_list[-drift_window:])
            mse_drift = float(end_mean - start_mean)
        else:
            mse_drift = 0.0
        stability_index = float(abs(mse_drift))
        
        metrics = {
            # é¢„æµ‹æ€§èƒ½
            'mse': float(mse),
            'mae': float(mae),
            'rmse': float(rmse),
            'rse': float(rse),
            'r2': float(r2),
            'mape': float(mape),
            
            # Proxy æ›´æ–°ç»Ÿè®¡
            'update_rate': float(update_rate),
            'anomaly_rate': float(anomaly_rate),
            'avg_proxy_loss': float(avg_proxy_loss),
            'total_updates': int(sum(self.update_flags)),
            'total_anomalies': len(self.anomaly_indices),
            
            # ç›‘ç£æ›´æ–°ç»Ÿè®¡
            'supervised_update_rate': float(supervised_update_rate),
            'avg_supervised_loss': float(avg_supervised_loss),
            'total_supervised_updates': int(sum(self.supervised_update_flags)),
            
            # æ—¶é—´ç»Ÿè®¡
            'avg_inference_time_ms': float(avg_inference_time * 1000),
            'avg_update_time_ms': float(avg_update_time * 1000),
            'avg_supervised_update_time_ms': float(avg_supervised_update_time * 1000),
            'avg_total_time_ms': float(total_time * 1000),
            'latency_p95_ms': float(latency_p95 * 1000),
            'throughput_steps_per_sec': float(throughput),
            
            # é—¨æ§ç»Ÿè®¡
            'avg_gate_value': float(np.mean(self.gate_values)) if self.gate_values else 0.0,
            
            # ç¨³å®šæ€§/æ³¢åŠ¨
            'mse_volatility': float(mse_volatility),
            'mse_drift': float(mse_drift),
            'stability_index': float(stability_index),
            
            # æ€»æ­¥æ•°
            'total_steps': self.total_steps
        }
        
        return metrics
    
    def get_trajectory(self) -> Dict[str, np.ndarray]:
        """
        è·å–å®Œæ•´çš„è½¨è¿¹æ•°æ® (ç”¨äºå¯è§†åŒ–)
        
        Returns:
            trajectory: åŒ…å«æ—¶é—´åºåˆ—æ•°æ®çš„å­—å…¸
        """
        return {
            'mse': np.array(self.mse_list),
            'mae': np.array(self.mae_list),
            'proxy_loss': np.array(self.proxy_losses),
            'update_flags': np.array(self.update_flags),
            'anomaly_indices': np.array(self.anomaly_indices),
            'supervised_update_flags': np.array(self.supervised_update_flags),
            'supervised_losses': np.array(self.supervised_losses),
            'inference_times': np.array(self.inference_times),
            'update_times': np.array(self.update_times),
            'supervised_update_times': np.array(self.supervised_update_times),
            'gate_values': np.array(self.gate_values)
        }


class SurpriseGate:
    """
    æƒŠå¥‡åº¦é—¨æ§ (Surprise Gate)
    
    æ ¹æ® Proxy Loss åˆ¤æ–­æ˜¯å¦åº”è¯¥æ›´æ–°æ¨¡å‹:
    - Loss æ­£å¸¸: æ›´æ–° (é€‚åº”æ¦‚å¿µæ¼‚ç§»)
    - Loss å¼‚å¸¸: è·³è¿‡ (é¿å…å¼‚å¸¸æ•°æ®æ±¡æŸ“)
    
    ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•ç¡®å®šé˜ˆå€¼:
        threshold = mean + k * std
    """
    
    def __init__(
        self, 
        threshold_std: float = 3.0,
        warmup_steps: int = 50,
        adaptive: bool = True,
        window_size: int = 100
    ):
        """
        Args:
            threshold_std: é˜ˆå€¼ç³»æ•° (å‡ å€æ ‡å‡†å·®)
            warmup_steps: é¢„çƒ­æ­¥æ•° (ç”¨äºä¼°è®¡åˆå§‹åˆ†å¸ƒ)
            adaptive: æ˜¯å¦è‡ªé€‚åº”è°ƒæ•´é˜ˆå€¼
            window_size: æ»‘åŠ¨çª—å£å¤§å° (ç”¨äºè‡ªé€‚åº”)
        """
        self.threshold_std = threshold_std
        self.warmup_steps = warmup_steps
        self.adaptive = adaptive
        self.window_size = window_size
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.loss_history = []
        self.threshold = None
        self.mean = None
        self.std = None
        
        # è®¡æ•°å™¨
        self.step_count = 0
    
    def update_statistics(self, loss: float):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.loss_history.append(loss)
        
        # ä¿æŒçª—å£å¤§å°
        if self.adaptive and len(self.loss_history) > self.window_size:
            self.loss_history = self.loss_history[-self.window_size:]
        
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        if len(self.loss_history) >= self.warmup_steps:
            self.mean = np.mean(self.loss_history)
            self.std = np.std(self.loss_history)
            self.threshold = self.mean + self.threshold_std * self.std
    
    def should_update(self, proxy_loss: float) -> Tuple[bool, Dict[str, float]]:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥æ›´æ–°
        
        Args:
            proxy_loss: å½“å‰çš„ä»£ç†æŸå¤±
        
        Returns:
            should_update: True è¡¨ç¤ºåº”è¯¥æ›´æ–°
            info: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        self.step_count += 1
        
        # é¢„çƒ­é˜¶æ®µ: æ€»æ˜¯æ›´æ–°
        if self.step_count <= self.warmup_steps:
            self.update_statistics(proxy_loss)
            return True, {
                'proxy_loss': proxy_loss,
                'threshold': None,
                'is_warmup': True
            }
        
        # æ­£å¸¸é˜¶æ®µ: æ ¹æ®é˜ˆå€¼åˆ¤æ–­
        self.update_statistics(proxy_loss)
        
        should_update = proxy_loss <= self.threshold
        
        info = {
            'proxy_loss': proxy_loss,
            'threshold': self.threshold,
            'mean': self.mean,
            'std': self.std,
            'is_warmup': False,
            'is_anomaly': not should_update
        }
        
        return should_update, info
    
    def reset(self):
        """é‡ç½®é—¨æ§çŠ¶æ€"""
        self.loss_history = []
        self.threshold = None
        self.mean = None
        self.std = None
        self.step_count = 0


def compute_threshold_from_validation(
    model,
    val_loader,
    device,
    percentile: float = 95.0
) -> float:
    """
    ä»éªŒè¯é›†è®¡ç®—æƒŠå¥‡åº¦é˜ˆå€¼
    
    Args:
        model: M-Stream æ¨¡å‹
        val_loader: éªŒè¯é›† DataLoader
        device: è®¾å¤‡
        percentile: ç™¾åˆ†ä½æ•° (ä¾‹å¦‚ 95 è¡¨ç¤ºå– 95% åˆ†ä½æ•°)
    
    Returns:
        threshold: é˜ˆå€¼
    """
    model.eval()
    proxy_losses = []
    
    with torch.no_grad():
        for batch_x, _, _, _ in val_loader:
            batch_x = batch_x.float().to(device)
            
            # è·å– embedding
            _, enc_out = model(batch_x, mode='online')
            
            # è®¡ç®— proxy loss
            proxy_loss = model.memory.compute_proxy_loss(enc_out)
            proxy_losses.append(proxy_loss.item())
    
    # è®¡ç®—é˜ˆå€¼
    threshold = np.percentile(proxy_losses, percentile)
    
    print(f"Validation Proxy Loss Statistics:")
    print(f"  Mean: {np.mean(proxy_losses):.6f}")
    print(f"  Std: {np.std(proxy_losses):.6f}")
    print(f"  {percentile}th percentile: {threshold:.6f}")
    
    return threshold


def visualize_online_results(
    metrics: OnlineMetrics,
    save_path: Optional[str] = None,
    show: bool = True
):
    """
    å¯è§†åŒ–åœ¨çº¿å­¦ä¹ ç»“æœ
    
    Args:
        metrics: OnlineMetrics å¯¹è±¡
        save_path: ä¿å­˜è·¯å¾„ (å¯é€‰)
        show: æ˜¯å¦æ˜¾ç¤ºå›¾åƒ
    """
    trajectory = metrics.get_trajectory()
    
    fig, axes = plt.subplots(3, 1, figsize=(12, 10))
    
    # 1. é¢„æµ‹è¯¯å·®éšæ—¶é—´å˜åŒ–
    ax1 = axes[0]
    steps = np.arange(len(trajectory['mse']))
    ax1.plot(steps, trajectory['mse'], label='MSE', alpha=0.7)
    ax1.plot(steps, trajectory['mae'], label='MAE', alpha=0.7)
    
    # æ ‡è®°å¼‚å¸¸ç‚¹
    if len(trajectory['anomaly_indices']) > 0:
        ax1.scatter(
            trajectory['anomaly_indices'],
            trajectory['mse'][trajectory['anomaly_indices']],
            color='red',
            marker='x',
            s=50,
            label='Anomaly (No Update)',
            zorder=5
        )
    
    ax1.set_xlabel('Time Step')
    ax1.set_ylabel('Prediction Error')
    ax1.set_title('Prediction Error Over Time')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Proxy Loss å’Œæ›´æ–°çŠ¶æ€
    ax2 = axes[1]
    ax2.plot(steps, trajectory['proxy_loss'], label='Proxy Loss', color='blue', alpha=0.7)
    
    # ç”¨é¢œè‰²åŒºåˆ†æ›´æ–°å’Œè·³è¿‡
    update_steps = steps[trajectory['update_flags']]
    skip_steps = steps[~trajectory['update_flags']]
    
    if len(update_steps) > 0:
        ax2.scatter(
            update_steps,
            trajectory['proxy_loss'][trajectory['update_flags']],
            color='green',
            marker='o',
            s=20,
            label='Updated',
            alpha=0.5
        )
    
    if len(skip_steps) > 0:
        ax2.scatter(
            skip_steps,
            trajectory['proxy_loss'][~trajectory['update_flags']],
            color='red',
            marker='x',
            s=30,
            label='Skipped',
            alpha=0.7
        )
    
    ax2.set_xlabel('Time Step')
    ax2.set_ylabel('Proxy Loss')
    ax2.set_title('Proxy Loss and Update Status')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_yscale('log')
    
    # 3. æ—¶é—´æ€§èƒ½
    ax3 = axes[2]
    ax3.plot(steps, trajectory['inference_times'] * 1000, label='Inference Time', alpha=0.7)
    ax3.plot(steps, trajectory['update_times'] * 1000, label='Update Time', alpha=0.7)
    ax3.set_xlabel('Time Step')
    ax3.set_ylabel('Time (ms)')
    ax3.set_title('Time Performance')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Figure saved to {save_path}")
    
    if show:
        plt.show()
    else:
        plt.close()


def print_online_summary(metrics_dict: Dict[str, float]):
    """
    æ‰“å°åœ¨çº¿å­¦ä¹ ç»“æœæ‘˜è¦
    
    Args:
        metrics_dict: compute() è¿”å›çš„æŒ‡æ ‡å­—å…¸
    """
    print("\n" + "="*60)
    print("Online Learning Results Summary")
    print("="*60)
    
    print("\nğŸ“Š Prediction Performance:")
    print(f"  MSE:  {metrics_dict['mse']:.6f}")
    print(f"  MAE:  {metrics_dict['mae']:.6f}")
    print(f"  RMSE: {metrics_dict['rmse']:.6f}")
    print(f"  RSE:  {metrics_dict['rse']:.6f}")
    print(f"  R2:   {metrics_dict['r2']:.6f}")
    print(f"  MAPE: {metrics_dict['mape']:.6f}")
    
    print("\nğŸ”„ Update Statistics:")
    print(f"  Total Steps:    {metrics_dict['total_steps']}")
    print(f"  Proxy Updates:  {metrics_dict['total_updates']} ({metrics_dict['update_rate']*100:.2f}%)")
    print(f"  Anomalies:      {metrics_dict['total_anomalies']} ({metrics_dict['anomaly_rate']*100:.2f}%)")
    print(f"  Avg Proxy Loss: {metrics_dict['avg_proxy_loss']:.6f}")
    
    if 'avg_gate_value' in metrics_dict:
        print(f"  Avg Gate Value: {metrics_dict['avg_gate_value']:.4f}")
    
    # ç›‘ç£æ›´æ–°ç»Ÿè®¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'total_supervised_updates' in metrics_dict and metrics_dict['total_supervised_updates'] > 0:
        print("\nğŸ¯ Supervised Update Statistics (Delayed Feedback):")
        print(f"  Total Supervised Updates: {metrics_dict['total_supervised_updates']}")
        print(f"  Supervised Update Rate:   {metrics_dict['supervised_update_rate']*100:.2f}%")
        print(f"  Avg Supervised Loss:      {metrics_dict['avg_supervised_loss']:.6f}")
    
    print("\nâ±ï¸  Time Performance:")
    print(f"  Avg Inference Time: {metrics_dict['avg_inference_time_ms']:.2f} ms/step")
    print(f"  Avg Proxy Update:   {metrics_dict['avg_update_time_ms']:.2f} ms/step")
    if 'avg_supervised_update_time_ms' in metrics_dict and metrics_dict['avg_supervised_update_time_ms'] > 0:
        print(f"  Avg Supervised Update: {metrics_dict['avg_supervised_update_time_ms']:.2f} ms/step")
    print(f"  Avg Total Time:     {metrics_dict['avg_total_time_ms']:.2f} ms/step")
    print(f"  Latency P95:        {metrics_dict['latency_p95_ms']:.2f} ms")
    print(f"  Throughput:         {metrics_dict['throughput_steps_per_sec']:.2f} steps/s")
    
    print("\nğŸ“‰ Stability Metrics:")
    print(f"  MSE Volatility:     {metrics_dict['mse_volatility']:.6f}")
    print(f"  MSE Drift:          {metrics_dict['mse_drift']:.6f}")
    print(f"  Stability Index:    {metrics_dict['stability_index']:.6f}")
    
    print("="*60 + "\n")


def save_online_results(
    metrics_dict: Dict[str, float],
    trajectory: Dict[str, np.ndarray],
    save_dir: str,
    setting: str,
    predictions: Optional[List[np.ndarray]] = None,
    targets: Optional[List[np.ndarray]] = None,
    channel_names: Optional[List[str]] = None
):
    """
    ä¿å­˜åœ¨çº¿å­¦ä¹ ç»“æœ
    
    Args:
        metrics_dict: æŒ‡æ ‡å­—å…¸
        trajectory: è½¨è¿¹æ•°æ®
        save_dir: ä¿å­˜ç›®å½•
        setting: å®éªŒè®¾ç½®åç§°
        predictions: é¢„æµ‹å€¼åˆ—è¡¨ (å¯é€‰)
        targets: çœŸå®å€¼åˆ—è¡¨ (å¯é€‰)
    """
    import os
    import json
    import pandas as pd
    
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜æŒ‡æ ‡ (JSON)
    metrics_path = os.path.join(save_dir, f'{setting}_metrics.json')
    with open(metrics_path, 'w') as f:
        json.dump(metrics_dict, f, indent=2)
    print(f"Metrics saved to {metrics_path}")
    
    # ä¿å­˜é¢„æµ‹ç»“æœä¸º CSV (å¦‚æœæä¾›)
    if predictions is not None and targets is not None:
        save_predictions_to_csv(predictions, targets, save_dir, setting, channel_names=channel_names)


def save_predictions_to_csv(
    predictions: List[np.ndarray],
    targets: List[np.ndarray],
    save_dir: str,
    setting: str,
    reduce_overlap: bool = True,
    channel_names: Optional[List[str]] = None
):
    """
    å°†é¢„æµ‹ç»“æœä¿å­˜ä¸º CSV æ–‡ä»¶
    
    Args:
        predictions: é¢„æµ‹å€¼åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º [B, H, C]
        targets: çœŸå®å€¼åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º [B, H, C]
        save_dir: ä¿å­˜ç›®å½•
        setting: å®éªŒè®¾ç½®åç§°
        reduce_overlap: æ˜¯å¦å‡å°‘é‡å  (åªä¿å­˜ Horizon=1 çš„ç‚¹ï¼Œä»¥åŠå°‘é‡çš„å®Œæ•´é¢„æµ‹)
    """
    import pandas as pd
    import os
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
    all_preds = np.concatenate(predictions, axis=0)  # [N, H, C]
    all_targets = np.concatenate(targets, axis=0)    # [N, H, C]
    
    N, H, C = all_preds.shape
    
    # 1. ä¿å­˜ "Streaming View" (Horizon=1) - æœ€å¸¸ç”¨
    # è¿™ä»£è¡¨äº†æ¨¡å‹åœ¨æ¯ä¸ªæ—¶é—´æ­¥å¯¹"ä¸‹ä¸€æ­¥"çš„é¢„æµ‹
    stream_rows = []
    channel_labels = [f'channel_{c}' for c in range(C)] if not channel_names else channel_names
    for t in range(N):
        for c in range(C):
            pred_val = all_preds[t, 0, c]
            true_val = all_targets[t, 0, c]
            stream_rows.append({
                'step': t,
                'channel': channel_labels[c],
                'pred': pred_val,
                'true': true_val,
                'error': pred_val - true_val,
                'abs_error': abs(pred_val - true_val)
            })
    
    stream_df = pd.DataFrame(stream_rows)
    stream_path = os.path.join(save_dir, f'{setting}_predictions_stream.csv')
    stream_df.to_csv(stream_path, index=False, float_format='%.6f')
    print(f"Streaming predictions (H=1) saved to {stream_path}")
    
    # 2. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ (True vs Pred)
    visualize_predictions_aligned(stream_df, save_dir, setting)


def visualize_predictions_aligned(stream_df, save_dir, setting):
    """
    å¯è§†åŒ–çœŸå®å€¼ä¸é¢„æµ‹å€¼æ›²çº¿ (åŸºäº H=1 çš„æµå¼é¢„æµ‹)
    """
    import matplotlib.pyplot as plt
    # import seaborn as sns
    
    # è·å–é€šé“åˆ—è¡¨
    channels = stream_df['channel'].unique()
    n_channels = len(channels)
    
    # æœ€å¤šç”» 4 ä¸ªé€šé“ï¼Œé¿å…å¤ªæ‹¥æŒ¤
    plot_channels = channels[:min(4, n_channels)]
    
    fig, axes = plt.subplots(len(plot_channels), 1, figsize=(15, 4 * len(plot_channels)), sharex=True)
    if len(plot_channels) == 1:
        axes = [axes]
        
    for ax, c in zip(axes, plot_channels):
        data = stream_df[stream_df['channel'] == c]
        
        ax.plot(data['step'], data['true'], label='Ground Truth', color='black', alpha=0.6, linewidth=1.5)
        ax.plot(data['step'], data['pred'], label='Prediction (H=1)', color='dodgerblue', alpha=0.8, linewidth=1.5)
        
        # è®¡ç®—è¯¥é€šé“çš„ MSE
        mse = (data['error'] ** 2).mean()
        ax.set_title(f'Channel {c} - MSE: {mse:.4f}')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # å±€éƒ¨æ”¾å¤§æ’å›¾ (Zoom-in)
        # é€‰å–ä¸­é—´ 100 ä¸ªç‚¹
        if len(data) > 200:
            start = len(data) // 2
            end = start + 100
            zoom_data = data.iloc[start:end]
            
            # åˆ›å»ºæ’å›¾
            ins = ax.inset_axes([0.6, 0.6, 0.35, 0.35])
            ins.plot(zoom_data['step'], zoom_data['true'], color='black', alpha=0.6)
            ins.plot(zoom_data['step'], zoom_data['pred'], color='dodgerblue', alpha=0.8)
            ins.set_title('Zoom (100 steps)', fontsize=8)
            ins.set_xticks([])
            ins.set_yticks([])
            
            # æŒ‡ç¤ºæ’å›¾ä½ç½®
            ax.indicate_inset_zoom(ins, edgecolor="black")
            
    plt.xlabel('Time Step')
    plt.tight_layout()
    
    fig_path = os.path.join(save_dir, f'{setting}_forecast_comparison.png')
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    print(f"Forecast comparison figure saved to {fig_path}")
    plt.close()


def visualize_online_results_enhanced(
    metrics: OnlineMetrics,
    save_dir: str,
    setting: str,
    show: bool = False
):
    """
    å¢å¼ºç‰ˆå¯è§†åŒ–åœ¨çº¿å­¦ä¹ ç»“æœï¼ˆç”Ÿæˆå¤šä¸ªå›¾è¡¨ï¼‰
    
    Args:
        metrics: OnlineMetrics å¯¹è±¡
        save_dir: ä¿å­˜ç›®å½•
        setting: å®éªŒè®¾ç½®åç§°
        show: æ˜¯å¦æ˜¾ç¤ºå›¾åƒ
    """
    import os
    os.makedirs(save_dir, exist_ok=True)
    
    trajectory = metrics.get_trajectory()
    metrics_summary = metrics.compute()
    steps = np.arange(len(trajectory['mse']))
    
    # ========== å›¾1: ç»¼åˆè§†å›¾ (4å­å›¾) ==========
    fig1, axes = plt.subplots(4, 1, figsize=(14, 16))
    
    # 1.1 é¢„æµ‹è¯¯å·®éšæ—¶é—´å˜åŒ–
    ax1 = axes[0]
    ax1.plot(steps, trajectory['mse'], label='MSE', alpha=0.7, linewidth=2)
    ax1.plot(steps, trajectory['mae'], label='MAE', alpha=0.7, linewidth=2)
    
    # æ ‡è®°å¼‚å¸¸ç‚¹
    if len(trajectory['anomaly_indices']) > 0:
        ax1.scatter(
            trajectory['anomaly_indices'],
            trajectory['mse'][trajectory['anomaly_indices']],
            color='red',
            marker='x',
            s=100,
            label='Anomaly (No Update)',
            zorder=5
        )
    
    ax1.set_xlabel('Time Step', fontsize=12)
    ax1.set_ylabel('Prediction Error', fontsize=12)
    ax1.set_title('Prediction Error Over Time', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    
    # 1.2 Proxy Loss å’Œæ›´æ–°çŠ¶æ€
    ax2 = axes[1]
    ax2.plot(steps, trajectory['proxy_loss'], label='Proxy Loss', color='blue', alpha=0.7, linewidth=2)
    
    # ç”¨é¢œè‰²åŒºåˆ†æ›´æ–°å’Œè·³è¿‡
    update_steps = steps[trajectory['update_flags']]
    skip_steps = steps[~trajectory['update_flags']]
    
    if len(update_steps) > 0:
        ax2.scatter(
            update_steps,
            trajectory['proxy_loss'][trajectory['update_flags']],
            color='green',
            marker='o',
            s=30,
            label='Updated',
            alpha=0.5
        )
    
    if len(skip_steps) > 0:
        ax2.scatter(
            skip_steps,
            trajectory['proxy_loss'][~trajectory['update_flags']],
            color='red',
            marker='x',
            s=50,
            label='Skipped (Anomaly)',
            alpha=0.8
        )
    
    ax2.set_xlabel('Time Step', fontsize=12)
    ax2.set_ylabel('Proxy Loss (log scale)', fontsize=12)
    ax2.set_title('Proxy Loss and Update Status', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)
    ax2.set_yscale('log')
    
    # 1.3 æ—¶é—´æ€§èƒ½
    ax3 = axes[2]
    ax3.plot(steps, trajectory['inference_times'] * 1000, label='Inference Time', alpha=0.7, linewidth=2)
    ax3.plot(steps, trajectory['update_times'] * 1000, label='Update Time', alpha=0.7, linewidth=2)
    total_times = (trajectory['inference_times'] + trajectory['update_times']) * 1000
    ax3.plot(steps, total_times, label='Total Time', alpha=0.7, linewidth=2, linestyle='--')
    
    ax3.set_xlabel('Time Step', fontsize=12)
    ax3.set_ylabel('Time (ms)', fontsize=12)
    ax3.set_title('Time Performance', fontsize=14, fontweight='bold')
    ax3.legend(fontsize=10)
    ax3.grid(True, alpha=0.3)
    
    # 1.4 é—¨æ§å€¼å˜åŒ– (Gate Value)
    ax4 = axes[3]
    if len(trajectory['gate_values']) > 0:
        ax4.plot(steps, trajectory['gate_values'], label='Gate Value (Memory Weight)', color='purple', alpha=0.7, linewidth=2)
        ax4.set_ylabel('Gate Value (0-1)', fontsize=12)
        ax4.set_ylim([0, 1.1])
        ax4.set_title('Dynamic Gate Evolution', fontsize=14, fontweight='bold')
        ax4.legend(fontsize=10)
        ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    fig1_path = os.path.join(save_dir, f'{setting}_overview.png')
    plt.savefig(fig1_path, dpi=300, bbox_inches='tight')
    print(f"Overview figure saved to {fig1_path}")
    if not show:
        plt.close()
    
    # ========== å›¾2: è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾ ==========
    fig2, axes2 = plt.subplots(1, 2, figsize=(14, 5))
    
    ax_mse = axes2[0]
    ax_mse.hist(trajectory['mse'], bins=50, alpha=0.7, color='blue', edgecolor='black')
    ax_mse.axvline(np.mean(trajectory['mse']), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(trajectory["mse"]):.4f}')
    ax_mse.axvline(np.median(trajectory['mse']), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(trajectory["mse"]):.4f}')
    ax_mse.set_xlabel('MSE', fontsize=12)
    ax_mse.set_ylabel('Frequency', fontsize=12)
    ax_mse.set_title('MSE Distribution', fontsize=14, fontweight='bold')
    ax_mse.legend(fontsize=10)
    ax_mse.grid(True, alpha=0.3)
    
    ax_mae = axes2[1]
    ax_mae.hist(trajectory['mae'], bins=50, alpha=0.7, color='orange', edgecolor='black')
    ax_mae.axvline(np.mean(trajectory['mae']), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(trajectory["mae"]):.4f}')
    ax_mae.axvline(np.median(trajectory['mae']), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(trajectory["mae"]):.4f}')
    ax_mae.set_xlabel('MAE', fontsize=12)
    ax_mae.set_ylabel('Frequency', fontsize=12)
    ax_mae.set_title('MAE Distribution', fontsize=14, fontweight='bold')
    ax_mae.legend(fontsize=10)
    ax_mae.grid(True, alpha=0.3)
    
    plt.tight_layout()
    fig2_path = os.path.join(save_dir, f'{setting}_error_distribution.png')
    plt.savefig(fig2_path, dpi=300, bbox_inches='tight')
    print(f"Error distribution figure saved to {fig2_path}")
    if not show:
        plt.close()
    
    # ========== å›¾3: æ»‘åŠ¨çª—å£ç»Ÿè®¡ ==========
    window_size = min(50, len(steps) // 10)
    if window_size > 1:
        fig3, axes3 = plt.subplots(2, 1, figsize=(14, 8))
        
        # è®¡ç®—æ»‘åŠ¨çª—å£å¹³å‡
        mse_rolling = np.convolve(trajectory['mse'], np.ones(window_size)/window_size, mode='valid')
        mae_rolling = np.convolve(trajectory['mae'], np.ones(window_size)/window_size, mode='valid')
        rolling_steps = steps[:len(mse_rolling)]
        
        ax_rolling1 = axes3[0]
        ax_rolling1.plot(rolling_steps, mse_rolling, label=f'MSE (window={window_size})', linewidth=2)
        ax_rolling1.plot(rolling_steps, mae_rolling, label=f'MAE (window={window_size})', linewidth=2)
        ax_rolling1.set_xlabel('Time Step', fontsize=12)
        ax_rolling1.set_ylabel('Error', fontsize=12)
        ax_rolling1.set_title(f'Rolling Average Error (Window Size: {window_size})', fontsize=14, fontweight='bold')
        ax_rolling1.legend(fontsize=10)
        ax_rolling1.grid(True, alpha=0.3)
        
        # æ›´æ–°ç‡æ»‘åŠ¨çª—å£
        update_rolling = np.convolve(trajectory['update_flags'].astype(float), np.ones(window_size)/window_size, mode='valid')
        
        ax_rolling2 = axes3[1]
        ax_rolling2.plot(rolling_steps, update_rolling * 100, linewidth=2, color='green')
        ax_rolling2.axhline(np.mean(trajectory['update_flags']) * 100, color='red', linestyle='--', linewidth=2, label=f'Overall: {np.mean(trajectory["update_flags"])*100:.1f}%')
        ax_rolling2.set_xlabel('Time Step', fontsize=12)
        ax_rolling2.set_ylabel('Update Rate (%)', fontsize=12)
        ax_rolling2.set_title(f'Rolling Update Rate (Window Size: {window_size})', fontsize=14, fontweight='bold')
        ax_rolling2.legend(fontsize=10)
        ax_rolling2.grid(True, alpha=0.3)
        ax_rolling2.set_ylim([0, 105])
        
        plt.tight_layout()
        fig3_path = os.path.join(save_dir, f'{setting}_rolling_statistics.png')
        plt.savefig(fig3_path, dpi=300, bbox_inches='tight')
        print(f"Rolling statistics figure saved to {fig3_path}")
        if not show:
            plt.close()
    
    # ========== å›¾4: æŒ‡æ ‡æ€»è§ˆè¡¨ ==========
    summary_fig, ax_summary = plt.subplots(figsize=(10, 4))
    ax_summary.axis('off')
    table_data = [
        ['Metric', 'Value', 'Metric', 'Value'],
        ['MSE', f"{metrics_summary['mse']:.4f}", 'MAE', f"{metrics_summary['mae']:.4f}"],
        ['RMSE', f"{metrics_summary['rmse']:.4f}", 'MSE Drift', f"{metrics_summary['mse_drift']:.4f}"],
        ['MSE Volatility', f"{metrics_summary['mse_volatility']:.4f}", 'Stability Index', f"{metrics_summary['stability_index']:.4f}"],
        ['Avg total time (ms)', f"{metrics_summary['avg_total_time_ms']:.2f}", 'Latency P95 (ms)', f"{metrics_summary['latency_p95_ms']:.2f}"],
        ['Throughput (step/s)', f"{metrics_summary['throughput_steps_per_sec']:.2f}", 'Proxy update rate (%)', f"{metrics_summary['update_rate']*100:.2f}"],
    ]
    summary_table = ax_summary.table(cellText=table_data, cellLoc='center', loc='center')
    summary_table.auto_set_font_size(False)
    summary_table.set_fontsize(10)
    summary_table.scale(1, 1.5)
    summary_path = os.path.join(save_dir, f'{setting}_metrics_summary.png')
    plt.savefig(summary_path, dpi=300, bbox_inches='tight')
    print(f"Metrics summary figure saved to {summary_path}")
    if not show:
        plt.close()
    
    if show:
        plt.show()


class MovingAverage:
    """ç§»åŠ¨å¹³å‡å·¥å…·"""
    
    def __init__(self, window_size: int = 10):
        self.window_size = window_size
        self.values = []
    
    def update(self, value: float) -> float:
        """æ›´æ–°å¹¶è¿”å›ç§»åŠ¨å¹³å‡"""
        self.values.append(value)
        if len(self.values) > self.window_size:
            self.values.pop(0)
        return np.mean(self.values)
    
    def get(self) -> float:
        """è·å–å½“å‰ç§»åŠ¨å¹³å‡"""
        return np.mean(self.values) if self.values else 0.0


class DelayedFeedbackBuffer:
    """
    å»¶è¿Ÿåé¦ˆç¼“å†²åŒº (Delayed Feedback Buffer)
    
    ç”¨äºåœ¨çº¿å­¦ä¹ ä¸­å¤„ç†å»¶è¿Ÿåˆ°è¾¾çš„çœŸå®æ ‡ç­¾ï¼š
    1. Replay Buffer: å­˜å‚¨ (X_t, t) ç­‰å¾…æ ‡ç­¾åˆ°è¾¾
    2. Ready Queue: å­˜å‚¨æ ‡ç­¾å·²åˆ°è¾¾çš„ (X_t, Y_t, t, emb_t)
    3. è‡ªé€‚åº”è§¦å‘: ç´¯ç§¯åˆ°é˜ˆå€¼æˆ–æ£€æµ‹åˆ°å¼‚å¸¸æ—¶æ‰¹é‡æ›´æ–°
    
    å·¥ä½œæµç¨‹:
    - æ—¶åˆ» t: æ·»åŠ æ ·æœ¬ (X_t, emb_t) åˆ° Replay Buffer
    - æ—¶åˆ» t+H: æ ‡ç­¾ Y_t åˆ°è¾¾ï¼Œç§»åŠ¨åˆ° Ready Queue
    - è§¦å‘æ¡ä»¶æ»¡è¶³æ—¶: ä» Ready Queue å–å‡ºæ‰¹é‡æ ·æœ¬è¿›è¡Œç›‘ç£æ›´æ–°
    """
    
    def __init__(
        self,
        pred_horizon: int,
        batch_size: int = 8,
        max_buffer_size: int = 200,
        max_wait_steps: int = 20,
        weight_decay: float = 0.05,
        supervised_weight: float = 0.7,
        weight_temperature: float = 1.0,
        anomaly_boost: float = 1.0,
        min_ready_for_anomaly: int = 1
    ):
        """
        Args:
            pred_horizon: é¢„æµ‹è§†é‡ (H)ï¼Œæ ‡ç­¾å»¶è¿Ÿåˆ°è¾¾çš„æ­¥æ•°
            batch_size: æ‰¹é‡æ›´æ–°çš„è§¦å‘é˜ˆå€¼
            max_buffer_size: æœ€å¤§ç¼“å†²åŒºå¤§å°
            max_wait_steps: å¼ºåˆ¶æ›´æ–°çš„æœ€å¤§ç­‰å¾…æ­¥æ•°
            weight_decay: æ ·æœ¬æ—¶é—´è¡°å‡ç‡ (Î»)
            supervised_weight: ç›‘ç£æŸå¤±çš„æƒé‡ (Î²)
        """
        self.pred_horizon = pred_horizon
        self.batch_size = batch_size
        self.max_buffer_size = max_buffer_size
        self.max_wait_steps = max_wait_steps
        self.weight_decay = weight_decay
        self.weight_temperature = max(weight_temperature, 1e-6)
        self.anomaly_boost = anomaly_boost
        self.min_ready_for_anomaly = max(1, min_ready_for_anomaly)
        self.supervised_weight = supervised_weight
        
        # Replay Buffer: å­˜å‚¨ç­‰å¾…æ ‡ç­¾çš„æ ·æœ¬
        # æ ¼å¼: {step: {'batch_x': tensor, 'enc_out': tensor, 'label': tensor or None, 'available_step': int}}
        self.replay_buffer = {}
        
        # Ready Queue: å­˜å‚¨å¯ç”¨äºç›‘ç£æ›´æ–°çš„æ ·æœ¬
        self.ready_queue = []
        
        # æ ‡ç­¾å¯èƒ½å…ˆäºæ ·æœ¬æˆ–æ ·æœ¬è¢«ç§»é™¤æ—¶æš‚å­˜
        self.pending_labels = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.current_step = 0
        self.last_update_step = 0
        self.total_supervised_updates = 0
        self.total_samples_used = 0
        self.anomaly_trigger_count = 0
        self.last_update_reason = ""
        self.last_is_anomaly = False
    
    def add_sample(self, step: int, batch_x: torch.Tensor, enc_out: Optional[torch.Tensor]):
        """
        æ·»åŠ æ ·æœ¬åˆ° Replay Buffer (ç­‰å¾…æ ‡ç­¾)
        
        Args:
            step: æ ·æœ¬æ‰€å±çš„æ—¶é—´æ­¥
            batch_x: è¾“å…¥æ•°æ® [B, L, C]
            enc_out: Encoder è¾“å‡º [B*C, N, D]
        """
        sample = {
            'batch_x': batch_x.detach().clone(),
            'enc_out': enc_out.detach().clone() if enc_out is not None else None,
            'label': None,
            'available_step': step + self.pred_horizon
        }
        
        if step in self.pending_labels:
            sample['label'] = self.pending_labels.pop(step)
        
        self.replay_buffer[step] = sample
        
        # é™åˆ¶ç¼“å†²åŒºå¤§å°
        if len(self.replay_buffer) > self.max_buffer_size:
            oldest_step = min(self.replay_buffer.keys())
            removed = self.replay_buffer.pop(oldest_step)
            # å¦‚æœç§»é™¤çš„æ ·æœ¬å·²ç»å¸¦æœ‰æ ‡ç­¾ï¼Œé¿å…ä¸¢å¤±æ ‡ç­¾ä¿¡æ¯
            if removed['label'] is not None:
                self.pending_labels[oldest_step] = removed['label']
    
    def add_label(self, step: int, batch_y: torch.Tensor):
        """
        ä¸ºæŒ‡å®šæ ·æœ¬æ·»åŠ çœŸå®æ ‡ç­¾
        
        Args:
            step: æ ·æœ¬æ‰€å±çš„æ—¶é—´æ­¥
            batch_y: çœŸå®æ ‡ç­¾ [B, L, C]
        """
        label = batch_y.detach().clone()
        
        if step in self.replay_buffer:
            self.replay_buffer[step]['label'] = label
        else:
            # æ ·æœ¬å¯èƒ½å› å®¹é‡é™åˆ¶è¢«ç§»é™¤ï¼Œæš‚å­˜æ ‡ç­¾ç­‰å¾…æ ·æœ¬è¡¥å›
            self.pending_labels[step] = label
    
    def advance_time(self, current_step: int):
        """
        æ ¹æ®å½“å‰æ—¶é—´æ¨è¿›ï¼Œå°†æ»¡è¶³å»¶è¿Ÿæ¡ä»¶çš„æ ·æœ¬ç§»åŠ¨åˆ° Ready Queue
        """
        ready_steps = [
            step for step, data in self.replay_buffer.items()
            if data['label'] is not None and current_step >= data['available_step']
        ]
        
        ready_steps.sort()
        
        for step in ready_steps:
            data = self.replay_buffer.pop(step)
            batch_x = data['batch_x']
            batch_y = data['label']
            enc_out = data['enc_out']
            
            # å°ºå¯¸å¯¹é½ï¼ˆå¤„ç†æœ«å°¾æ‰¹æ¬¡ï¼‰
            if len(batch_y) < len(batch_x):
                new_batch_size = len(batch_y)
                batch_x = batch_x[:new_batch_size]
                if enc_out is not None:
                    old_batch_size = len(batch_x)
                    C = batch_x.shape[2] if len(batch_x.shape) == 3 else 1
                    if enc_out.shape[0] == old_batch_size * C:
                        enc_out = enc_out[:new_batch_size * C]
                    elif enc_out.shape[0] == old_batch_size:
                        enc_out = enc_out[:new_batch_size]
            
            if len(batch_x) < len(batch_y):
                batch_y = batch_y[:len(batch_x)]
            
            self.ready_queue.append((step, batch_x, batch_y, enc_out))
            
            if len(self.ready_queue) > self.max_buffer_size:
                self.ready_queue.pop(0)
    
    def should_update(
        self, 
        current_step: int, 
        is_anomaly: bool = False
    ) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘ç›‘ç£æ›´æ–°
        
        è§¦å‘æ¡ä»¶ (æ»¡è¶³ä»»ä¸€å³å¯):
        1. Ready Queue è¾¾åˆ°æ‰¹é‡å¤§å°
        2. æ£€æµ‹åˆ°å¼‚å¸¸ (ç´§æ€¥æ›´æ–°)
        3. è·ç¦»ä¸Šæ¬¡æ›´æ–°è¶…è¿‡æœ€å¤§ç­‰å¾…æ­¥æ•°
        
        Args:
            current_step: å½“å‰æ—¶é—´æ­¥
            is_anomaly: æ˜¯å¦æ£€æµ‹åˆ°å¼‚å¸¸
        
        Returns:
            should_update: æ˜¯å¦åº”è¯¥æ›´æ–°
        """
        self.current_step = current_step
        
        # æ¡ä»¶ 1: æ‰¹é‡å¤§å°è¾¾åˆ°é˜ˆå€¼
        if len(self.ready_queue) >= self.batch_size:
            return True
        
        # æ¡ä»¶ 2: æ£€æµ‹åˆ°å¼‚å¸¸ä¸”æœ‰å¯ç”¨æ ·æœ¬
        if is_anomaly and len(self.ready_queue) >= self.min_ready_for_anomaly:
            self.anomaly_trigger_count += 1
            self.last_is_anomaly = True
            self.last_update_reason = "anomaly"
            return True
        
        # æ¡ä»¶ 3: è¶…æ—¶å¼ºåˆ¶æ›´æ–°
        steps_since_update = current_step - self.last_update_step
        if steps_since_update >= self.max_wait_steps and len(self.ready_queue) > 0:
            self.last_is_anomaly = False
            self.last_update_reason = "timeout"
            return True
        
        self.last_is_anomaly = False
        self.last_update_reason = ""
        return False
    
    def get_batch(self) -> Optional[Tuple[List, np.ndarray]]:
        """
        ä» Ready Queue è·å–ä¸€æ‰¹æ ·æœ¬è¿›è¡Œæ›´æ–°
        
        Returns:
            batch_data: [(step, batch_x, batch_y, enc_out), ...] æˆ– None
            weights: æ—¶é—´è¡°å‡æƒé‡ [N] æˆ– None
        """
        if len(self.ready_queue) == 0:
            return None, None
        
        # å–å‡ºå…¨éƒ¨æˆ–éƒ¨åˆ†æ ·æœ¬ï¼ˆæœ€å¤š batch_size ä¸ªï¼‰
        # v5.0 Fix: å¦‚æœæ˜¯æœ€åä¸€æ‰¹ï¼ˆä¸”æ•°é‡è¾ƒå°‘ï¼‰ï¼Œå…è®¸å–å‡ºæ‰€æœ‰æ ·æœ¬
        num_samples = min(len(self.ready_queue), self.batch_size * 2)  # å…è®¸ç¨å¾®å¤šä¸€ç‚¹
        batch_data = self.ready_queue[:num_samples]
        self.ready_queue = self.ready_queue[num_samples:]
        
        # è®¡ç®—æ—¶é—´è¡°å‡æƒé‡ + æ¸©åº¦ + å¼‚å¸¸åŠ æƒ
        # weight_i = softmax(-Î» * Î”t / T)
        weights = []
        for step, _, _, _ in batch_data:
            time_diff = self.current_step - step
            weight = np.exp(-self.weight_decay * time_diff / self.weight_temperature)
            weights.append(weight)
        
        weights = np.array(weights, dtype=np.float64)
        if self.last_is_anomaly and self.anomaly_boost != 1.0:
            weights = weights * self.anomaly_boost
        
        # å½’ä¸€åŒ–æƒé‡ (é¿å…é™¤é›¶é”™è¯¯)
        weights_sum = weights.sum()
        if weights_sum > 0:
            weights = weights / weights_sum
        else:
            weights = np.ones_like(weights) / len(weights)
        
        # æ›´æ–°ç»Ÿè®¡
        self.last_update_step = self.current_step
        self.total_supervised_updates += 1
        self.total_samples_used += len(batch_data)
        
        return batch_data, weights
    
    def get_statistics(self) -> Dict[str, float]:
        """
        è·å–ç¼“å†²åŒºç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return {
            'replay_buffer_size': len(self.replay_buffer),
            'ready_queue_size': len(self.ready_queue),
            'total_supervised_updates': self.total_supervised_updates,
            'total_samples_used': self.total_samples_used,
            'avg_samples_per_update': (
                self.total_samples_used / self.total_supervised_updates
                if self.total_supervised_updates > 0 else 0.0
            ),
            'steps_since_last_update': self.current_step - self.last_update_step,
            'anomaly_trigger_count': self.anomaly_trigger_count,
            'last_update_reason': self.last_update_reason,
            'last_is_anomaly': self.last_is_anomaly
        }
    
    def reset(self):
        """é‡ç½®ç¼“å†²åŒº"""
        self.replay_buffer.clear()
        self.ready_queue.clear()
        self.pending_labels.clear()
        self.current_step = 0
        self.last_update_step = 0
        self.total_supervised_updates = 0
        self.total_samples_used = 0


def visualize_memory_keys_similarity(
    model,
    save_path: Optional[str] = None,
    show: bool = True
):
    """
    v6.0: å¯è§†åŒ– Memory Keys çš„ç›¸ä¼¼åº¦çŸ©é˜µçƒ­åŠ›å›¾
    
    ç”¨äºç›‘æ§æ­£äº¤æ€§çº¦æŸæ˜¯å¦ç”Ÿæ•ˆï¼Œæ£€æµ‹ Mode Collapse
    
    Args:
        model: M-Stream æ¨¡å‹å®ä¾‹
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        show: æ˜¯å¦æ˜¾ç¤ºå›¾åƒ
    """
    if not hasattr(model, 'memory'):
        print("âš ï¸  Model does not have a memory module.")
        return
    
    memory = model.memory
    if not hasattr(memory, 'memory_keys'):
        print("âš ï¸  Memory module does not have memory_keys parameter.")
        return
    
    # è®¡ç®—å½’ä¸€åŒ–åçš„ç›¸ä¼¼åº¦çŸ©é˜µ
    with torch.no_grad():
        keys = memory.memory_keys  # [M, D]
        keys_norm = torch.nn.functional.normalize(keys, p=2, dim=1)
        similarity_matrix = torch.matmul(keys_norm, keys_norm.t())  # [M, M]
        similarity_np = similarity_matrix.cpu().numpy()
    
    num_prototypes = similarity_np.shape[0]
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    # å¯¹è§’çº¿å…ƒç´ åº”è¯¥æ¥è¿‘ 1ï¼Œéå¯¹è§’çº¿åº”è¯¥æ¥è¿‘ 0
    diag_mean = np.diag(similarity_np).mean()
    off_diag_mask = ~np.eye(num_prototypes, dtype=bool)
    off_diag_mean = np.abs(similarity_np[off_diag_mask]).mean()
    off_diag_max = np.abs(similarity_np[off_diag_mask]).max()
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    fig, ax = plt.subplots(figsize=(10, 8))
    
    im = ax.imshow(similarity_np, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')
    
    # æ·»åŠ è‰²æ¡
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('Cosine Similarity', fontsize=12)
    
    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
    ax.set_title(
        f'Memory Keys Similarity Matrix (v6.0)\n'
        f'Diag: {diag_mean:.3f} | Off-Diag Mean: {off_diag_mean:.3f} | Off-Diag Max: {off_diag_max:.3f}',
        fontsize=14, fontweight='bold'
    )
    ax.set_xlabel('Memory Prototype Index', fontsize=12)
    ax.set_ylabel('Memory Prototype Index', fontsize=12)
    
    # æ·»åŠ ç½‘æ ¼
    ax.set_xticks(np.arange(0, num_prototypes, max(1, num_prototypes // 10)))
    ax.set_yticks(np.arange(0, num_prototypes, max(1, num_prototypes // 10)))
    ax.grid(False)
    
    # æ·»åŠ æ–‡æœ¬æ³¨é‡Šï¼ˆä»…åœ¨åŸå‹æ•°é‡è¾ƒå°‘æ—¶ï¼‰
    if num_prototypes <= 16:
        for i in range(num_prototypes):
            for j in range(num_prototypes):
                text = ax.text(j, i, f'{similarity_np[i, j]:.2f}',
                             ha="center", va="center", color="black", fontsize=8)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ“ Memory similarity matrix saved to {save_path}")
    
    if show:
        plt.show()
    else:
        plt.close()
    
    # è¿”å›ç»Ÿè®¡ä¿¡æ¯
    return {
        'diagonal_mean': float(diag_mean),
        'off_diagonal_mean': float(off_diag_mean),
        'off_diagonal_max': float(off_diag_max),
        'orthogonality_score': 1.0 - float(off_diag_mean)  # è¶Šæ¥è¿‘ 1 è¶Šå¥½
    }
